{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Step 6: Generating Paragraphs for Topic Modeled Tweets\n",
    "\n",
    "In this section, we'll generate paragraphs for a set of tweets based on topic modeling results. The tweets have been preprocessed, clustered, and labeled manually. The main steps include:\n",
    "\n",
    "#### 1. Cluster Relatedness Calculation\n",
    "- Read the codebook and clustered tweets with embeddings.\n",
    "- Determine the relatedness of each cluster to the event by calculating the percentage of related tweets.\n",
    "- Create a DataFrame of clusters and their relatedness.\n",
    "\n",
    "#### 2. Vectorization and Topic Modeling\n",
    "- Vectorize the text of related tweets using CountVectorizer with n-grams.\n",
    "- Use a guided topic modeling approach with seed words from the codebook to categorize tweets into topics.\n",
    "\n",
    "#### 3. Topic Predictions for Test Set\n",
    "- Read a test set for topic modeling.\n",
    "- Tokenize and vectorize the test set tweets.\n",
    "- Predict the top 5 categories for each tweet based on the trained topic model.\n",
    "\n",
    "#### 4. Embedding and Paragraph Generation\n",
    "- Use BERT embeddings for both tweets and predefined sentences.\n",
    "- Generate paragraphs for each tweet based on the predicted categories.\n",
    "- Calculate the average cosine similarity between selected sentences in the generated paragraphs.\n",
    "\n",
    "#### 5. Output\n",
    "- Save the original tweets and generated paragraphs to a CSV file for further analysis.\n",
    "\n",
    "**Note**: The resulting CSV file, 'step_6_paragraphs.csv', contains the original tweets and the generated paragraphs.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8220effa6ef35a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from lda import guidedlda\n",
    "from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer\n",
    "\n",
    "from utils import enhanced_stop_words, preprocess_tweet, calculate_topic_modeling_score, \\\n",
    "    get_pretrained_model_and_tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "codebook = pd.read_csv('../data/intermediate/input/step_5_codebook_manually_labeled.csv')\n",
    "tweets_df = pickle.load(open('../data/intermediate/input/step_4_clustered_tweets_with_embeddings.pkl', 'rb'))\n",
    "labeled_clusters_df = pd.read_csv('../data/intermediate/input/step_4_cluster_samples_manually_labeled.csv')\n",
    "# for each cluster sum is_related column and divide by the number of tweets in the cluster\n",
    "# to get the percentage of tweets that are related to the event\n",
    "cluster_relatedness_df = labeled_clusters_df.groupby('cluster').agg({'is_related': 'sum', 'id': 'count'}).reset_index()\n",
    "cluster_relatedness_df['relatedness'] = cluster_relatedness_df['is_related'] / cluster_relatedness_df['id']\n",
    "cluster_relatedness_df['is_related'] = cluster_relatedness_df['relatedness'] > 0.5\n",
    "cluster_relatedness_df = cluster_relatedness_df[['cluster', 'is_related']]\n",
    "related_tweets_df = tweets_df[tweets_df['cluster'].isin(cluster_relatedness_df[cluster_relatedness_df['is_related']]['cluster'].tolist())]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d50e79ce0eeb8a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dictionary of codebook as key is the category and value is the list of n_grams\n",
    "codebook_dict = {x: codebook.loc[codebook['category'] == x, 'n_gram'].tolist() for x in codebook['category'].unique()}\n",
    "category_ids = {x: i for i, x in enumerate(codebook['category'].unique())}\n",
    "category_ids_inv = {i: x for i, x in enumerate(codebook['category'].unique())}\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), min_df=0.001, max_df=0.85, stop_words=enhanced_stop_words)\n",
    "v_fit = vectorizer.fit_transform(related_tweets_df['processed_text'].tolist())\n",
    "word2id = dict((v, idx) for idx, v in enumerate(vectorizer.get_feature_names_out()))\n",
    "\n",
    "seed_topics = {}\n",
    "for cagegory, seed_words in codebook_dict.items():\n",
    "    for word in seed_words:\n",
    "        if word not in word2id:\n",
    "            continue\n",
    "        seed_topics[word2id[word]] = category_ids[cagegory]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e326de18b12a68b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TOPIC_NUMBER = 10\n",
    "NITER = 25\n",
    "ALPHA = .3 #\n",
    "ETA = .05\n",
    "CONF = 1\n",
    "IN_OR_OUT = 1\n",
    "TOP_N_WORDS = 20\n",
    "window_sizes = [0, 1, 2, 5]\n",
    "\n",
    "model = guidedlda.GuidedLDA(\n",
    "        n_topics=TOPIC_NUMBER,\n",
    "        n_iter=NITER,\n",
    "        random_state=0,\n",
    "        alpha=ALPHA,\n",
    "        eta=ETA\n",
    "    )\n",
    "model.fit(v_fit, seed_topics=seed_topics, seed_confidence=CONF)\n",
    "test_set_df = pd.read_csv('../data/test_set_for_topic_modeling.csv')\n",
    "test_set_df['processed_text'] = test_set_df['text'].apply(preprocess_tweet)\n",
    "test_set_df['vectorized_text'] = test_set_df['processed_text'].apply(lambda x: vectorizer.transform([x]))\n",
    "# get top5 topics for each tweet as cat1_pred, cat2_pred, cat3_pred, cat4_pred, cat5_pred\n",
    "test_set_df['topic_predictions'] = test_set_df['vectorized_text'].apply(lambda x: model.transform(x)[0])\n",
    "for i in range(1,6):\n",
    "    test_set_df[f'cat{i}_pred'] = test_set_df['topic_predictions'].apply(lambda x: category_ids_inv.get(np.argsort(x)[-i],f\"temp{np.argsort(x)[-i]-7}\"))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a39fdd68008c04c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predefined_sentences_df = pd.read_csv('../data/predefined_sentences.csv')\n",
    "strategy = \"source_tweet\" # source_tweet or previous_sentence\n",
    "model_name = 'bert' # options are bert, roberta, sbert, sroberta\n",
    "model, tokenizer = get_pretrained_model_and_tokenizer(model_name)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "predefined_sentences_df['processed_sentence'] = predefined_sentences_df['sentence'].apply(preprocess_tweet)\n",
    "predefined_sentences_df['embedding'] = predefined_sentences_df['processed_sentence'].apply(lambda x: model(**tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True).to(device))['last_hidden_state'].mean(dim=0).detach().cpu().numpy())\n",
    "\n",
    "test_set_df['embedding'] = test_set_df['processed_text'].apply(lambda x: model(**tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True).to(device))['last_hidden_state'].mean(dim=0).detach().cpu().numpy())\n",
    "paragraphs = []\n",
    "paragraph_cos_sim = []\n",
    "for idx,row in test_set_df.iterrows():\n",
    "    selected_sentences = []\n",
    "    # for each category get the highest cosine similarity sentences from predefined sentences df with the same category\n",
    "    for i in range(1,6):\n",
    "        cat = row[f'cat{i}_pred']\n",
    "        if strategy == \"previous_sentence\" and selected_sentences:\n",
    "            cos_sim = predefined_sentences_df[predefined_sentences_df['category'] == cat]['embedding'].apply(lambda x: np.dot(x, selected_sentences[-1]['embedding']) / (np.linalg.norm(x) * np.linalg.norm(selected_sentences[-1]['embedding']))).tolist()\n",
    "        else:\n",
    "            cos_sim = predefined_sentences_df[predefined_sentences_df['category'] == cat]['embedding'].apply(lambda x: np.dot(x, row['embedding']) / (np.linalg.norm(x) * np.linalg.norm(row['embedding']))).tolist()\n",
    "        selected_sentences.append(predefined_sentences_df[predefined_sentences_df['category'] == cat].iloc[np.argmax(cos_sim)])\n",
    "    \n",
    "    # calculate cosine similarity between the selected sentences\n",
    "    selected_sentences_cos_sims = [np.dot(x['embedding'], y['embedding']) / (np.linalg.norm(x['embedding']) * np.linalg.norm(y['embedding'])) for x,y in zip(selected_sentences, selected_sentences[1:])]\n",
    "    \n",
    "    paragraphs.append(\". \".join([x['sentence'] for x in selected_sentences]))\n",
    "    paragraph_cos_sim.append(np.mean(selected_sentences_cos_sims))\n",
    "\n",
    "paragraph_cos_sim = np.mean(paragraph_cos_sim)\n",
    "\n",
    "test_set_df['paragraph'] = paragraphs\n",
    "\n",
    "test_set_df[['text', 'paragraph']].to_csv('../data/intermediate/output/step_6_paragraphs.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "334fd38ed79ac50f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
