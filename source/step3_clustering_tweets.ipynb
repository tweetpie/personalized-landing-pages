{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Step3: Analyzing Twitter Data with BERT Embeddings and Clustering\n",
    "\n",
    "In this notebook, we analyze a collection of tweets using BERT embeddings and clustering techniques. The main steps include loading the tweet data, preprocessing the text, obtaining BERT embeddings, performing K-means clustering, calculating silhouette scores, and visualizing the clusters in 3D space.\n",
    "\n",
    "### Process Overview\n",
    "- Load the tweet data from the CSV file 'tweets.csv'.\n",
    "- Preprocess the tweet text using the 'preprocess_tweet' function.\n",
    "- Use the BERT model ('bert-base-uncased') to obtain embeddings for each tweet.\n",
    "- Apply K-means clustering with a specified number of clusters (n_clusters).\n",
    "- Calculate the silhouette score to assess the clustering quality.\n",
    "- Reduce the dimensionality of the embeddings to 3D using PCA for visualization.\n",
    "- Sample 400 tweets from each cluster for visualization purposes.\n",
    "- Create 3D scatter plots of the sampled tweets from different angles.\n",
    "\n",
    "**Note**: The resulting DataFrame includes the original tweet data, processed text, BERT embeddings, cluster assignments, silhouette scores, and 3D PCA coordinates for visualization. (intermediate/output/step_3_cluster_samples.csv)\n",
    "**Note2**: The 'is_related' column in the resulting DataFrame is left blank and should be filled by domain experts based on their knowledge. (intermediate/input/step_4_cluster_samples_manually_labeled.csv)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27a4c9e397aab0c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from utils import preprocess_tweet, get_pretrained_model_and_tokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('../data/tweets.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23c772a3f23948b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets_df['processed_text'] = tweets_df['text'].apply(preprocess_tweet)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6117555838d28680"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = 'bert' # options are bert, roberta, sbert, sroberta\n",
    "model, tokenizer = get_pretrained_model_and_tokenizer(model_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12d92b0b12430e50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "tweets_df['embedding'] = None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acb4493dffb483ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_texts = tweets_df['processed_text'].tolist()\n",
    "batch_size = 1000\n",
    "embeddings = []\n",
    "for i in tqdm(range(0, len(processed_texts), batch_size)):\n",
    "    inputs = tokenizer(processed_texts[i:i+batch_size], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    model = model.to(device)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings.extend(outputs['last_hidden_state'].to('cpu').mean(dim=1).detach().numpy().tolist())\n",
    "tweets_df['embedding'] = embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1dde5977ce7fe48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pickle.dump(tweets_df, open('../data/tweets_with_embeddings.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "409a3cf99360e27d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_clusters = 150\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans.fit(tweets_df['embedding'].tolist())\n",
    "tweets_df['cluster'] = kmeans.labels_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "658b32edf061e7ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate silhouette score\n",
    "silh_score = silhouette_score(tweets_df['embedding'].tolist(), tweets_df['cluster'].tolist())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "904d0fc49ad11634"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reduce dimension to 3d using PCA for visualization\n",
    "pca = PCA(n_components=3, random_state=0)\n",
    "tweets_df['embedding_3d'] = pca.fit_transform(np.array(tweets_df['embedding'].tolist()), np.array(tweets_df['cluster'].tolist())).tolist()\n",
    "# get sample 400 tweets from each cluster to visualize\n",
    "sampled_tweets_df = tweets_df.groupby('cluster').apply(lambda x: x.sample(100, replace=True)).reset_index(drop=True)\n",
    "\n",
    "# draw 3d plot in 3 different angles using matplotlib\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(sampled_tweets_df['embedding_3d'].apply(lambda x: x[0]*100).tolist(), sampled_tweets_df['embedding_3d'].apply(lambda x: x[1]*100).tolist(), sampled_tweets_df['embedding_3d'].apply(lambda x: x[2]*100).tolist(), c=sampled_tweets_df['cluster'], cmap='tab20c')\n",
    "ax.view_init(0, 0)\n",
    "plt.show()\n",
    "\n",
    "ax.view_init(90, 0)\n",
    "plt.show()\n",
    "\n",
    "ax.view_init(180, 0)\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e32990fef93548a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# get top 1000 tweets from each cluster that are closest to the centroid\n",
    "tweets_df['distance_to_centroid'] = tweets_df['embedding'].apply(lambda x: np.linalg.norm(x - centroids[kmeans.predict([x])[0]]))\n",
    "top_tweets_df = tweets_df.groupby('cluster').apply(lambda x: x.sort_values('distance_to_centroid').head(100)).reset_index(drop=True)\n",
    "top_tweets_df = top_tweets_df[['id', 'text', 'created_at', 'processed_text', 'cluster']]\n",
    "top_tweets_df.to_csv('../data/intermediate/output/step_3_cluster_samples.csv', index=False)\n",
    "top_tweets_df['is_related'] = None\n",
    "top_tweets_df.to_csv('../data/intermediate/input/step_4_cluster_samples_manually_labeled.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c9bf53c3667abe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pickle.dump(tweets_df, open('../data/intermediate/input/step_4_clustered_tweets_with_embeddings.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94b310272f10b1b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
