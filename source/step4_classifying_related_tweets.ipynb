{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Step4: Analyzing Twitter Data for Event-Related Clusters\n",
    "\n",
    "In this notebook, we will analyze a dataset of tweets to identify event-related clusters. The primary steps include loading the data, preprocessing the tweets, determining the relatedness of clusters, extracting relevant n-grams, and predicting the relatedness of a test set using BERT embeddings.\n",
    "\n",
    "### Process Overview\n",
    "- Load the pre-clustered tweets and manually labeled cluster samples.\n",
    "- Calculate the percentage of related tweets in each cluster.\n",
    "- Generate a summary table for related and non-related clusters, including cluster size, text length, and word count statistics.\n",
    "- Extract n-grams from related clusters using CountVectorizer to build the codebook.\n",
    "- Save n-grams information to a CSV file and create a codebook for manual labeling.\n",
    "- Use BERT embeddings to predict the relatedness of a test set and evaluate the model performance.\n",
    "\n",
    "**Note**: Make sure to label the generated codebook, n-grams CSV file further steps."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b0aa4a10716fd18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils import preprocess_tweet, get_pretrained_model_and_tokenizer\n",
    "import swifter\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from IPython.display import display, Markdown\n",
    "import torch\n",
    "from source.utils import enhanced_stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from source.lm_classifier.main import pipeline\n",
    "from source.lm_classifier.main import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets_df = pickle.load(open('../data/intermediate/input/step_4_clustered_tweets_with_embeddings.pkl', 'rb'))\n",
    "labeled_clusters_df = pd.read_csv('../data/intermediate/input/step_4_cluster_samples_manually_labeled.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1657a59fffd4e680"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for each cluster sum is_related column and divide by the number of tweets in the cluster\n",
    "# to get the percentage of tweets that are related to the event\n",
    "cluster_relatedness_df = labeled_clusters_df.groupby('cluster').agg({'is_related': 'sum', 'id': 'count'}).reset_index()\n",
    "cluster_relatedness_df['relatedness'] = cluster_relatedness_df['is_related'] / cluster_relatedness_df['id']\n",
    "cluster_relatedness_df['is_related'] = cluster_relatedness_df['relatedness'] > 0.5\n",
    "cluster_relatedness_df = cluster_relatedness_df[['cluster', 'is_related']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44caad8de8bb0564"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for is_related true and false clusters print the summary with following information \n",
    "# number of clusters, number of tweets, min cluster size, max cluster size, avg. cluster size, min text length, max text length, avg. text length, min word count, max word count, avg. word count\n",
    "\n",
    "tweets_df['text_length'] = tweets_df['text'].apply(lambda x: len(x))\n",
    "tweets_df['word_count'] = tweets_df['text'].apply(lambda x: len(x.split(' ')))\n",
    "tweets_df['cluster_size'] = tweets_df['cluster'].map(tweets_df['cluster'].value_counts())\n",
    "\n",
    "summary_df = tweets_df.merge(cluster_relatedness_df, on='cluster', how='left')\n",
    "summary_df = summary_df.groupby('is_related').agg({'cluster': 'nunique', 'id': 'count', 'cluster_size': ['min', 'max', 'mean'], 'text_length': ['min', 'max', 'mean'], 'word_count': ['min', 'max', 'mean']}).reset_index()\n",
    "\n",
    "# print df as markdown table\n",
    "display(Markdown(summary_df.T.to_markdown()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "441fcbc68320a889"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get only related clusters\n",
    "related_tweets_df = tweets_df[tweets_df['cluster'].isin(cluster_relatedness_df[cluster_relatedness_df['is_related']]['cluster'].tolist())]\n",
    "# get n-grams up to 3-grams and keep n-grams having 0.01 min. document-frequency and 0.85 max. document-frequency \n",
    "# to get rid of very common and very rare n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), min_df=0.001, max_df=0.85, stop_words=enhanced_stop_words)\n",
    "v_fit = vectorizer.fit_transform(related_tweets_df['processed_text'].tolist())\n",
    "\n",
    "# create a df with n-grams and their document-frequency\n",
    "n_grams_df = pd.DataFrame({'n_gram': vectorizer.get_feature_names_out(), 'document_frequency': v_fit.toarray().sum(axis=0)}).sort_values('document_frequency', ascending=False)\n",
    "n_grams_df.to_csv('../data/intermediate/output/step_4_n_grams.csv', index=False)\n",
    "n_grams_df['synonym'] = None\n",
    "n_grams_df['category'] = None\n",
    "n_grams_df[['n_gram', 'synonym', 'category']].to_csv('../data/intermediate/input/step_5_codebook_manually_labeled.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ec3e6112068d1bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = 'sbert' # options are bert, roberta, sbert, sroberta\n",
    "model, tokenizer = get_pretrained_model_and_tokenizer(model_name)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ce88056a8ad74a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('../data/test_set_for_relatedness.csv')\n",
    "test_set['processed_text'] = test_set['text'].swifter.apply(preprocess_tweet)\n",
    "processed_texts = test_set['processed_text'].tolist()\n",
    "inputs = tokenizer(processed_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "model = model.to(device)\n",
    "outputs = model(**inputs)\n",
    "test_set['embedding'] = outputs['last_hidden_state'].to('cpu').mean(dim=1).detach().numpy().tolist()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd456725fc060eed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get cluster centers for each cluster as dict\n",
    "tweets_df['embedding'] = tweets_df['embedding'].apply(lambda x: np.array(x))\n",
    "cluster_centers = tweets_df.groupby('cluster').agg({'embedding': 'mean'}).to_dict()['embedding']\n",
    "closest_cluster_count = 7\n",
    "thresholds = [4./closest_cluster_count, 5./closest_cluster_count]\n",
    "# predict cluster for each tweet in test set by calculating cosine similarity between tweet embedding and cluster centers\n",
    "# get closest 7 clusters and assign label of majority as related or not related\n",
    "test_set['cluster'] = test_set['embedding'].apply(lambda x: sorted(cluster_centers.keys(), key=lambda y: np.dot(x, cluster_centers[y]), reverse=True)[:closest_cluster_count])\n",
    "for threshold in thresholds:\n",
    "    test_set['is_related_prediction'] = test_set['cluster'].apply(lambda x: cluster_relatedness_df[cluster_relatedness_df['cluster'].isin(x)]['is_related'].mean() >= threshold).astype(int)\n",
    "    \n",
    "    # print precision, recall, accuracy and f1 by comparing label and is_related_prediction columns\n",
    "    precision = test_set[(test_set['label'] == 1) & (test_set['is_related_prediction'] == 1)].shape[0] / test_set[test_set['is_related_prediction'] == 1].shape[0]\n",
    "    recall = test_set[(test_set['label'] == 1) & (test_set['is_related_prediction'] == 1)].shape[0] / test_set[test_set['label'] == 1].shape[0]\n",
    "    accuracy = test_set[test_set['label'] == test_set['is_related_prediction']].shape[0] / test_set.shape[0]\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(f'Closest cluster count: {closest_cluster_count}, Threshold: {threshold}, Precision: {precision}, Recall: {recall}, Accuracy: {accuracy}, F1: {f1}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcf95fa524c24b45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = tweets_df[['id', 'text', 'cluster']]\n",
    "dataset['label'] = dataset['cluster'].isin(cluster_relatedness_df[cluster_relatedness_df['is_related']]['cluster'].tolist()).astype(int)\n",
    "\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "dataset = dataset[['id', 'text', 'label']]\n",
    "# lm-based classifier is implemented on top of following repo: https://github.com/raufer/text-classification-pytorch\n",
    "\n",
    "modelname = 'vanillabert' # options are vanillabert, sbert, roberta, sroberta\n",
    "config = {\n",
    "    'num-epochs-pretrain': 5,\n",
    "    'num-epochs-train': 5,\n",
    "    'learning-rate-pretrain': 9e-4,\n",
    "    'learning-rate-train': 2e-5,\n",
    "    'dropout-ratio': 0.4,\n",
    "    'threshold': 0.5, # 0.5 or 0.95\n",
    "}\n",
    "\n",
    "model, y_true, y_pred, output_path, train_dataset, val_dataset, test_dataset = pipeline(\n",
    "    datapath=dataset,\n",
    "    modelname=modelname,\n",
    "    output_dir='data/outputs',\n",
    "    config=config\n",
    ")\n",
    "\n",
    "score = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"weighted f1-score '{score}'\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c5fb6e7e19cd60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('../data/test_set_for_relatedness.csv')\n",
    "predict(modelname, test_set, output_path, threshold=config['threshold'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a19bbef31b8c7a0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
